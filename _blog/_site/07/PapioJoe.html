<h2 id="what-are-some-lessons-learned-from-this-case">What are some lessons learned from this case?</h2>

<p>I think one really important thing to take away from the <em>LaCour</em> case study is that biases in how and why and when and what academic work gets published (or even befor that, funded) creates somewhat perverse incentives which push some to consider and even commit academic fraud.</p>

<p><strong><em>The bias towards novelty</em></strong> incentivizes people to do things for the first time; if I try something new, that’s exciting, I have a decent shot at getting it published. But if I try to duplicate someone else’s work to verify their findings, that’s less exciting. “I tried so and so’s methods and got the same results” is a much less exciting read, despite being just as, if not more, important as the initial findings in the integrity of the process of empirical discovery. An increase in scholars making efforts at the reproduction of other people’s work for the sake of verification would increase the likelihood that fraudulent methods, data, and findings would be discovered, which is a powerful deterrent to the temptation to commit such falsification in the first place.</p>

<p><strong><em>The bias towards “significance”</em></strong> incentivizes people to publish “things that work”. If I try something and get a <em>p-value</em> that is significant at whatever \alpha level, that’s exciting and probably have a good shot at getting it published. But if I try something and I get a <em>p-value</em> that is “not significant”, that’s not very exciting and I’m probably don’t have a good shot publication. But “I did something and it didn’t work”, just like “I tried what someone else did and got the same results”, is a crucial part of the discovery process. Many important “successful” discoveries happened only many many “failures”. I would be curious to know how many times “failed” research has been duplicated because the “failed” results were never published. The mind reels at how much time and effort may have been wasted because of this.</p>

<p>I don’t have good solutions to either of these problems, but I think part of the answer lies in changing the incentives in academic publishing and funding. Reproduction and failure at reproduction, while not as exciting as novelty, needs to be given more import as such practices are crucial to maintaining the integrity of the discovery process and empirical methods. And I think part of the answer lies in journal editors and grant awarders encouraging more “failed” experiments with so called “non significant” <em>p-values</em>.</p>

<p>My rant being complete, I now cede the soapbox.</p>
