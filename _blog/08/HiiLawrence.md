---
title: "Reproducibility"
author: "Lawrence Hii"
topic: "09"
layout: post
root: ../../../
---

Both of the articles emphasized on the importance of reproducibility. In Donoho's paper, Mr.Claerbout put his foot down on the reproducibility research. He "pointed out that the research deliverable is not an image itself, but instead the software environment that, applied in the right way, produces the image, and which, hopefully, could be applied to other datasets to produce equally nice images" which is true. The results aren't anything if the algorithm is missing or unknown towards audience. One of the greatest example that pointed here for both papers is the Wavelab. Due to the transparency of the algorithm Wavelab in 90's, it has been widely been used by lot's of people. 

And it is in the second paper that Donoho reevaluated the situation of reproducible research. In the second paper, as the technology develops, we can see that computational analysis gains more momentum for higher dimension analysis which makes the second paper different from the initial paper. We relied more on the computational science, we are not only look at the transparency or reproducibility research but also cross-section platform (Linus, Mac, Windows and others). With that, it gives another reason on why we as analyst should get all our work to be able to reproduce, at best from the scratch. It not only helps to your team members but also future analysts. 

One thing that I take away from these two papers is that as an analyst, we always try to take the shortcut or the shortest amount of time to finish our work. Because of this, we tend to hard coded our work and shut down the reproducibility of our work or the worst case is turning our work in to a black box which is a nightmare.
