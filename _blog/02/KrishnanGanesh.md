
---
title: "Split-Apply-Combine..."
author: "Ganesh Krishnan"
topic: "02"
layout: post
root: ../../../
---
Which (base R) functions do you know that support the split-apply-combine strategy? In your opinion, are these sufficient - state why or why not?

The Base R functions that I know of for this strategy are the set of apply functions including apply, lapply, sapply, aggregate along with the traditional looping tratgy. I often use the dataframe command to shorten typically large datasets into subsets with only relevant columns and rows. After my carrying out my computations I have generally made use of the rbind, cbind commands to merge data sets. Often these base R functions are not enough. Looping is a very simple and clear way of doing things if you are new to R, but it is also very very time taking and often cannot be used if the data to be analysed is of a very big size. Apply functions are helpul but when it comes to using 3D datasets I have found them to become increasingly complicated to understand, and verify. For fairly simple tasks although, I believe the base R functions are enough, especially when we are dealing with uncomplicated datasets that have small number of rows and columns. As the data sets in the real world problems are not that simple and have many issues, I believe making use of only base R functions would take out the focus from data analysis and put more focus on the how to achieve it part.


Using a dataset of your choice, describe how you can use the split-apply-combine strategy for a prt of the data analysis.

In my recent work I have been working on a large data set which contains around 200 columns. The work involves conducting energy analysis on 5 different buildings by finding average monthly, seasonal, quaterly and yearly temperature, power consumption changes and its correlation with occupancy of the building rooms, time of the day, and outside temperature. With an intention of finding out the traditional method of Heating, Ventilation and air conditioning is better or worse than some brand new technologies of doing the same.  The data set is for one year, and has data with energy, power, fan velocity, and various component specific details like temperature, pressure in a unit, space occupancy etc.

For each building site, I typically first combine daily data into an enormous dataset that represents one year worth of data and which has one row representing one minute of readings for more than 200 columns. The combining task is important for me as it enables me to perform task and check the whole dataset for any errors which I can address before I start processing the data. Then I divide my task according to the kind of analysis required. If I need power calculations, I first remove all columns that are not required by using apply functions if the filtering depends on a criteria. I also use the base dataframe command if I know the column names or number to be removed. Then I use the apply and aggregate functions again to perform any computations on the subset of the dataset. Sometimes I need this dataset along with other columns like fan speed from the original raw data set, in which case I simply filter out the columns from the original data set along with the time and combine my data sets using the Timestamp as a Key column, I have often used the Join command from the Plyr package for doing this as it provides the easiest and most hasselfree way for me to achieve data set merger. Thus, I can safely say that I have already started utilizing the split-apply-combine strategy and have found it immensely helpful especially when using it on very large data sets, as it not only allows me to keep track and be on top of the calculations I need to make but also makes managing and handling the large dataset very easy.

I have often also needed to make monthly, or weekly calculations, for which I generally compute on the reduced large dataset and simply split the data weekwise, or sometimes when not compuationally permissible , I first split the data weekwise or month wise or season wise as required and then do my computations and then create a new dataframe/ datatable with all my data combined in it.
